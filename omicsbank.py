# -*- coding: utf-8 -*-
"""OmicsBank.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14XfeK6VtsAnJWqmXZeVs55_xjfRkzE2N

# Note:Generated output for only the first row of diagnosis list since qouta limit of my free gemini api key
"""

# Core LangChain and related packages
!pip install langchain
!pip install langchain-community
!pip install langchain-google-genai

# Vector database
!pip install chromadb

# Embeddings
!pip install sentence-transformers

# Additional dependencies that might be needed
!pip install transformers
!pip install torch
!pip install numpy
!pip install pandas

# For better text processing
!pip install nltk
!pip install spacy

# If you encounter any SSL or connection issues
!pip install requests
!pip install urllib3

# Upgrade pip to latest version
!pip install --upgrade pip

import pandas as pd

# Initialize
rows = []
current_chapter = None
current_chapter_desc = None
current_block = None
current_block_desc = None

# Read the ICD-10 txt file
with open("/content/drive/MyDrive/code-description pairs.txt", "r", encoding="utf-8") as f:
    for line in f:
        line = line.strip()
        if not line:
            continue  # skip empty lines

        parts = line.split("\t")

        # Case 1: Chapter (e.g., "I\tCertain infectious and parasitic diseases")
        if len(parts) == 2 and not any(char.isdigit() for char in parts[0]):
            current_chapter = parts[0].strip()
            current_chapter_desc = parts[1].strip()

        # Case 2: Block (e.g., "A00-A09\tIntestinal infectious diseases")
        elif len(parts) == 2 and "-" in parts[0]:
            current_block = parts[0].strip()
            current_block_desc = parts[1].strip()

        # Case 3: Actual ICD-10 code (e.g., A00, A00.0, A00.01, etc.)
        elif len(parts) == 2:
            code = parts[0].strip()
            desc = parts[1].strip()
            rows.append({
                "chapter": current_chapter,
                "chapter_description": current_chapter_desc,
                "block": current_block,
                "block_description": current_block_desc,
                "code": code,
                "code_description": desc
            })

df = pd.DataFrame(rows)
print(df.head(10))
df.to_csv('icd_10_codes', index=False)

import pandas as pd
import ast
import re
import os
import time
import logging
import asyncio
from typing import Dict, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.docstore.document import Document
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache
from tqdm import tqdm
import numpy as np
from functools import lru_cache
import pickle

logging.basicConfig(
    filename="processing.log",
    filemode="a",
    format="%(asctime)s - %(levelname)s - %(message)s",
    level=logging.INFO
)

set_llm_cache(InMemoryCache())

class OptimizedICD10Mapper:
    def __init__(self, icd_df: pd.DataFrame, google_api_key: str, persist_directory: str = "./chroma_icd_db"):
        self.icd_df = icd_df
        self.google_api_key = google_api_key
        self.persist_directory = persist_directory
        self.embedding_model = HuggingFaceEmbeddings(
            model_name="all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        self.vectorstore = None
        self.llm = None
        self.fallback_count = 0
        self.fallback_logs = []
        self.timing_logs = []
        self.cache_file = "diagnosis_cache.pkl"
        self.diagnosis_cache = self._load_cache()

        self.enhanced_fallback_mappings = self._create_enhanced_fallback_mappings()

        self.max_workers = min(32, os.cpu_count() + 4)

        self.setup_pipeline()

    def _load_cache(self) -> Dict:
        """Load cached diagnosis mappings"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                logging.warning(f"Failed to load cache: {e}")
        return {}

    def _save_cache(self):
        """Save diagnosis mappings to cache"""
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.diagnosis_cache, f)
        except Exception as e:
            logging.warning(f"Failed to save cache: {e}")

    def _create_enhanced_fallback_mappings(self) -> Dict[str, str]:
        """Create more comprehensive fallback mappings"""
        base_mappings = {
            # Cardiovascular
            "hypertension": "I10", "high blood pressure": "I10",
            "heart failure": "I50.9", "cardiac arrest": "I46.9",
            "myocardial infarction": "I21.9", "heart attack": "I21.9",
            "arrhythmia": "I49.9", "atrial fibrillation": "I48.91",

            # Respiratory
            "asthma": "J45.9", "copd": "J44.1", "pneumonia": "J18.9",
            "respiratory failure": "J96.90", "shortness of breath": "R06.02",
            "cough": "R05", "bronchitis": "J20.9",

            # Endocrine/Metabolic
            "diabetes": "E11.9", "diabetes mellitus": "E11.9",
            "type 2 diabetes": "E11.9", "hyperglycemia": "R73.9",
            "hypoglycemia": "E16.2", "obesity": "E66.9",

            # Neurological
            "stroke": "I63.9", "seizure": "G40.909", "epilepsy": "G40.909",
            "dementia": "F03.90", "alzheimer": "G30.9",
            "headache": "R51.9", "migraine": "G43.909",

            # Mental Health
            "depression": "F32.9", "anxiety": "F41.9", "bipolar": "F31.9",
            "schizophrenia": "F20.9", "ptsd": "F43.10",

            # Musculoskeletal
            "arthritis": "M19.90", "back pain": "M54.9", "joint pain": "M25.50",
            "fracture": "S72.90XA", "osteoporosis": "M81.0",

            # Gastrointestinal
            "nausea": "R11.2", "vomiting": "R11.10", "diarrhea": "K59.1",
            "constipation": "K59.00", "abdominal pain": "R10.9",

            # Genitourinary
            "kidney disease": "N28.9", "uti": "N39.0", "incontinence": "R32",

            # General symptoms
            "fever": "R50.9", "fatigue": "R53.83", "weakness": "R53.1",
            "dizziness": "R42", "syncope": "R55", "chest pain": "R07.9",
            "pain": "R52"
        }

        enhanced_mappings = {}
        for key, value in base_mappings.items():
            enhanced_mappings[key] = value
            enhanced_mappings[key.replace(" ", "")] = value
            enhanced_mappings[key.replace("-", " ")] = value

        return enhanced_mappings

    def setup_pipeline(self):
        """Setup the retrieval and LLM pipeline with optimizations"""
        self.icd_df["enhanced_content"] = (
            self.icd_df["code"] + " | " +
            self.icd_df["code_description"] + " | " +
            self.icd_df["code_description"].str.lower() + " | " +
            self.icd_df["code_description"].str.replace(r'[^\w\s]', '', regex=True).str.lower()
        )

        docs = [
            Document(
                page_content=row["enhanced_content"],
                metadata={
                    "code": row["code"],
                    "description": row["code_description"],
                    "index": i
                }
            )
            for i, row in self.icd_df.iterrows()
        ]

        if os.path.exists(self.persist_directory):
            logging.info("Loading existing Chroma vector store...")
            self.vectorstore = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embedding_model
            )
        else:
            logging.info("Creating new Chroma vector store...")
            self.vectorstore = Chroma.from_documents(
                docs,
                embedding=self.embedding_model,
                persist_directory=self.persist_directory
            )

        # Optimized LLM setup
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            temperature=0.1,
            google_api_key=self.google_api_key,
            max_retries=2,
            request_timeout=30
        )

    def create_enhanced_prompt(self, diagnosis: str, retrieved_context: str) -> str:
        """Improved prompt template for better accuracy"""
        return f"""You are an expert medical coder specializing in ICD-10 classification. Your task is to map diagnoses to the most appropriate ICD-10 code with high accuracy.

DIAGNOSIS TO CODE: "{diagnosis}"

RELEVANT ICD-10 OPTIONS:
{retrieved_context}

CODING GUIDELINES:
1. Match the diagnosis to the MOST SPECIFIC available code
2. Consider anatomical location, severity, and clinical context
3. Prefer specific codes over unspecified ones when details are available
4. For complex diagnoses, focus on the primary condition
5. Must select from the provided options only

OUTPUT FORMAT (strictly follow):
ICD_CODE: [exact code]
DESCRIPTION: [exact description]
JUSTIFICATION: [brief explanation of match reasoning]
CONFIDENCE: [High/Medium/Low]

Response:"""

    @lru_cache(maxsize=1000)
    def _cached_similarity_search(self, diagnosis: str, k: int = 15) -> Tuple[List[str], str]:
        """Cached similarity search for repeated diagnoses"""
        retrieved_docs = self.vectorstore.similarity_search(diagnosis, k=k)
        retrieved_codes = [doc.metadata["code"] for doc in retrieved_docs]

        retrieved_context = "\n".join([
            f"Code: {doc.metadata['code']} | Description: {doc.metadata['description']}"
            for doc in retrieved_docs
        ])

        return retrieved_codes, retrieved_context

    def extract_icd_info_from_response(self, response: str) -> Dict[str, str]:
        """Enhanced parsing with multiple fallback strategies"""
        try:
            response = response.strip()

            patterns = {
                'code': r"ICD_CODE:\s*([A-Z]\d{1,2}(?:\.\d{1,3})?)",
                'description': r"DESCRIPTION:\s*([^\n]+)",
                'justification': r"JUSTIFICATION:\s*([^\n]+(?:\n(?!ICD_CODE:|DESCRIPTION:|CONFIDENCE:)[^\n]+)*)",
                'confidence': r"CONFIDENCE:\s*([^\n]+)"
            }

            extracted = {}
            for key, pattern in patterns.items():
                match = re.search(pattern, response, re.IGNORECASE)
                extracted[key] = match.group(1).strip() if match else "N/A"

            if extracted['code'] == "N/A":
                code_patterns = [
                    r"([A-Z]\d{1,2}(?:\.\d{1,3})?)",
                    r"Code:\s*([A-Z]\d{1,2}(?:\.\d{1,3})?)",
                    r"ICD.*?([A-Z]\d{1,2}(?:\.\d{1,3})?)"
                ]

                for pattern in code_patterns:
                    match = re.search(pattern, response)
                    if match:
                        extracted['code'] = match.group(1)
                        break

            return {
                "icd_code": extracted['code'],
                "description": extracted['description'],
                "justification": extracted['justification'],
                "confidence": extracted['confidence'],
                "raw_response": response
            }

        except Exception as e:
            return {
                "icd_code": "ERROR",
                "description": "Parsing failed",
                "justification": f"Error: {str(e)}",
                "confidence": "Low",
                "raw_response": response
            }

    def get_enhanced_fallback_code(self, diagnosis: str) -> Dict[str, str]:
        """Enhanced fallback with better matching"""
        self.fallback_count += 1
        diagnosis_lower = diagnosis.lower().strip()

        cleaned_diagnosis = re.sub(r'\b(acute|chronic|severe|mild|primary|secondary)\b', '', diagnosis_lower)
        cleaned_diagnosis = re.sub(r'\s+', ' ', cleaned_diagnosis).strip()

        for keyword, code in self.enhanced_fallback_mappings.items():
            if keyword in diagnosis_lower or keyword in cleaned_diagnosis:
                match = self.icd_df[self.icd_df["code"] == code]
                if not match.empty:
                    result = {
                        "icd_code": code,
                        "description": match.iloc[0]["code_description"],
                        "justification": f"Enhanced fallback match: {keyword}",
                        "confidence": "Medium",
                        "raw_response": "Enhanced_Fallback"
                    }
                    self.fallback_logs.append({**result, "diagnosis": diagnosis})
                    return result

        result = {
            "icd_code": "R69",
            "description": "Illness, unspecified",
            "justification": "Generic fallback - no specific match found",
            "confidence": "Low",
            "raw_response": "Generic_Fallback"
        }
        self.fallback_logs.append({**result, "diagnosis": diagnosis})
        return result

    def map_diagnosis_with_rag(self, diagnosis: str) -> Dict[str, str]:
        """Optimized RAG mapping with caching"""

        cache_key = diagnosis.lower().strip()
        if cache_key in self.diagnosis_cache:
            cached_result = self.diagnosis_cache[cache_key].copy()
            cached_result["from_cache"] = True
            return cached_result

        try:
            # Retrieve relevant documents
            retrieved_codes, retrieved_context = self._cached_similarity_search(diagnosis)

            # Create and send prompt
            prompt = self.create_enhanced_prompt(diagnosis, retrieved_context)
            response = self.llm.invoke(prompt)

            # Handle response
            response_text = response.content if hasattr(response, 'content') else str(response)
            result = self.extract_icd_info_from_response(response_text)

            result["original_diagnosis"] = diagnosis
            result["retrieved_icd_codes"] = ", ".join(retrieved_codes)
            result["from_cache"] = False

            # Validate result
            if result["icd_code"] in ["N/A", "ERROR"]:
                fallback = self.get_enhanced_fallback_code(diagnosis)
                fallback["original_diagnosis"] = diagnosis
                fallback["retrieved_icd_codes"] = ", ".join(retrieved_codes)
                fallback["from_cache"] = False

                # Cache the result
                self.diagnosis_cache[cache_key] = fallback.copy()
                return fallback

            # Cache successful result
            self.diagnosis_cache[cache_key] = result.copy()
            return result

        except Exception as e:
            logging.error(f"RAG mapping failed for '{diagnosis}': {str(e)}")
            fallback = self.get_enhanced_fallback_code(diagnosis)
            fallback["original_diagnosis"] = diagnosis
            fallback["justification"] = f"RAG failed: {str(e)}"
            fallback["retrieved_icd_codes"] = "Unavailable"
            fallback["from_cache"] = False

            # Cache the fallback
            self.diagnosis_cache[cache_key] = fallback.copy()
            return fallback

    def process_single_diagnosis(self, diagnosis_data: Tuple[int, str]) -> Dict[str, str]:
        """Process a single diagnosis - for parallel execution"""
        row_idx, diagnosis = diagnosis_data
        result = self.map_diagnosis_with_rag(diagnosis)
        result["patient_row"] = row_idx
        return result

    def process_diagnosis_dataset(self, diagnosis_df: pd.DataFrame, batch_size: int = 50) -> pd.DataFrame:
        """Optimized parallel processing with batching"""
        diagnosis_df["Diagnoses_list"] = diagnosis_df["Diagnoses_list"].apply(ast.literal_eval)

        all_diagnoses = [
            (i, diag.strip())
            for i, row in diagnosis_df.iterrows()
            for diag in row["Diagnoses_list"]
        ]

        results = []
        total_diagnoses = len(all_diagnoses)

        logging.info(f"Processing {total_diagnoses} diagnoses with {self.max_workers} workers")

        # Process in batches to manage memory and provide progress updates
        for batch_start in range(0, total_diagnoses, batch_size):
            batch_end = min(batch_start + batch_size, total_diagnoses)
            batch = all_diagnoses[batch_start:batch_end]

            start_time = time.time()

            # Parallel processing for the batch
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_diagnosis = {
                    executor.submit(self.process_single_diagnosis, diag_data): diag_data
                    for diag_data in batch
                }

                batch_results = []
                for future in tqdm(as_completed(future_to_diagnosis),
                                 total=len(batch),
                                 desc=f"Batch {batch_start//batch_size + 1}"):
                    try:
                        result = future.result(timeout=60)
                        batch_results.append(result)
                    except Exception as e:
                        diagnosis_data = future_to_diagnosis[future]
                        logging.error(f"Failed to process {diagnosis_data}: {str(e)}")
                        # Create error result
                        error_result = {
                            "icd_code": "ERROR",
                            "description": "Processing failed",
                            "justification": f"Processing error: {str(e)}",
                            "confidence": "Low",
                            "original_diagnosis": diagnosis_data[1],
                            "patient_row": diagnosis_data[0],
                            "from_cache": False
                        }
                        batch_results.append(error_result)

            results.extend(batch_results)

            # Log batch completion
            batch_time = time.time() - start_time
            processed_count = batch_end
            logging.info(f"Processed {processed_count}/{total_diagnoses} diagnoses in {batch_time:.2f} sec")

            self.timing_logs.append({
                "Batch": batch_start // batch_size + 1,
                "Diagnoses_Processed": processed_count,
                "Time_Seconds": round(batch_time, 2),
                "Cache_Hit_Rate": sum(1 for r in batch_results if r.get("from_cache", False)) / len(batch_results)
            })

        logging.info(f"Processing complete. Total fallbacks: {self.fallback_count}")
        logging.info(f"Cache size: {len(self.diagnosis_cache)}")

        # Save cache for future use
        self._save_cache()

        return pd.DataFrame(results)

    def save_logs(self):
        """Enhanced logging with performance metrics"""
        if self.fallback_logs:
            fallback_df = pd.DataFrame(self.fallback_logs)
            fallback_df.to_csv("fallbacks.csv", index=False)
            logging.info(f"Fallbacks saved: {len(self.fallback_logs)} entries")

        if self.timing_logs:
            timing_df = pd.DataFrame(self.timing_logs)
            timing_df.to_csv("timing_log.csv", index=False)
            logging.info("Timing log saved with cache performance metrics")

    def get_performance_stats(self) -> Dict[str, any]:
        """Get performance statistics"""
        return {
            "cache_size": len(self.diagnosis_cache),
            "fallback_count": self.fallback_count,
            "total_processed": len(self.diagnosis_cache) + self.fallback_count,
            "cache_hit_rate": len(self.diagnosis_cache) / max(1, len(self.diagnosis_cache) + self.fallback_count),
            "vectorstore_size": self.vectorstore._collection.count() if self.vectorstore else 0
        }

icd_df = pd.read_csv("/content/drive/MyDrive/icd_10_codes")
# Load diagnosis dataset
diagnosis_df = pd.read_csv("/content/drive/MyDrive/Diagnoses_list - Sheet1.csv").iloc[[0]]

# Your Google API key
GOOGLE_API_KEY = "AIzaSyBSGxahQHSVJbi3Gpqw_IgLNIzF4Gb445Y"
mapper = OptimizedICD10Mapper(icd_df, GOOGLE_API_KEY)

# Process with performance monitoring
results_df = mapper.process_diagnosis_dataset(diagnosis_df, batch_size=100)

# Get performance statistics
stats = mapper.get_performance_stats()
print(f"Cache hit rate: {stats['cache_hit_rate']:.2%}")

# Save enhanced logs
mapper.save_logs()

results_df

mapper.get_performance_stats()

timing_enhaced_df=pd.read_csv("/content/timing_log.csv")

timing_enhaced_df

results_df.to_csv('output.csv', index=False)